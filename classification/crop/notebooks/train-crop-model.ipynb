{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f003cc1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook captures the implementation of training an image classification model to identify if a crop is infected based on the input leaf image. This notebook will be executed in kaggle because of large data size as well as the various options we would like to explore for improving the training accuracy \n",
    "- Different Learning Rates : Warm-up LR, Step Decay LR, Cosine Decay LR, Hybrid, Annealing, Adaptive Decay LR\n",
    "- Different Cost Functions : Use Focal Loss function\n",
    "- Label Smoothening : Instead of a 0 and 1 for the labels create a continuous distribution of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0403662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from collections import Counter\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0699b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../input/cropdata/input/train/'\n",
    "TEST_DIR = '../input/cropdata/input/train/'\n",
    "VALIDATE_DIR = '../input/cropdata/input/train/'\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2adb4a",
   "metadata": {},
   "source": [
    "#### For GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6e1378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4690b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97921d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None, augment=False):\n",
    "        xform = transforms.Compose([transforms.ToTensor()]) if transform is None else transform\n",
    "        self.img_dataset = datasets.ImageFolder(file_path, transform=xform)\n",
    "        self.aug = augment\n",
    "        \n",
    "        ## augumentation initialization\n",
    "        ia.seed(241)\n",
    "\n",
    "        self.seq = iaa.Sequential([\n",
    "            iaa.Fliplr(0.5), # horizontal flips\n",
    "            iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "            # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "            # But we only blur about 50% of all images.\n",
    "            iaa.Sometimes(\n",
    "                0.5,\n",
    "                iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "            ),\n",
    "            # Strengthen or weaken the contrast in each image.\n",
    "            iaa.LinearContrast((0.75, 1.5)),\n",
    "                        \n",
    "            # Make some images brighter and some darker.\n",
    "            # In 20% of all cases, we sample the multiplier once per channel,\n",
    "            # which can end up changing the color of the images.\n",
    "            iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "            # Apply affine transformations to each image.\n",
    "            # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "            iaa.Affine(\n",
    "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                rotate=(-25, 25),\n",
    "                shear=(-8, 8)\n",
    "            )\n",
    "        ], random_order=True) # apply augmenters in random order\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.img_dataset[idx]\n",
    "        return image, label\n",
    "    \n",
    "    # this is an experimental method created to explore how the dunder methods can be modified\n",
    "    def __repr__(self, idx=0):\n",
    "        image, label = self.img_dataset[idx]\n",
    "        plt.imshow(image.permute(1,2,0))\n",
    "        return str(\"Display view of the image\")\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images, targets = list(zip(*batch))\n",
    "        images = torch.stack(images).permute(0 , 2, 3, 1)\n",
    "        if self.aug: images=self.seq.augment_images(images=images.numpy()) \n",
    "        targets = torch.tensor(targets)\n",
    "        images = torch.tensor(images).permute(0, 3, 1, 2)\n",
    "        if torch.cuda.is_available():\n",
    "            targets = targets.to(device)\n",
    "            images = images.to(device)\n",
    "        return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca32b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple___Apple_scab': 0, 'Apple___Black_rot': 1, 'Apple___Cedar_apple_rust': 2, 'Apple___healthy': 3, 'Blueberry___healthy': 4, 'Cherry_(including_sour)___Powdery_mildew': 5, 'Cherry_(including_sour)___healthy': 6, 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7, 'Corn_(maize)___Common_rust_': 8, 'Corn_(maize)___Northern_Leaf_Blight': 9, 'Corn_(maize)___healthy': 10, 'Grape___Black_rot': 11, 'Grape___Esca_(Black_Measles)': 12, 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13, 'Grape___healthy': 14, 'Peach___Bacterial_spot': 15, 'Peach___healthy': 16, 'Pepper,_bell___Bacterial_spot': 17, 'Pepper,_bell___healthy': 18, 'Potato___Early_blight': 19, 'Potato___Late_blight': 20, 'Potato___healthy': 21, 'Strawberry___Leaf_scorch': 22, 'Strawberry___healthy': 23, 'Tomato___Bacterial_spot': 24, 'Tomato___Early_blight': 25, 'Tomato___Late_blight': 26, 'Tomato___Leaf_Mold': 27, 'Tomato___Septoria_leaf_spot': 28, 'Tomato___Spider_mites Two-spotted_spider_mite': 29, 'Tomato___Target_Spot': 30, 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 31, 'Tomato___Tomato_mosaic_virus': 32, 'Tomato___healthy': 33}\n"
     ]
    }
   ],
   "source": [
    "# Here we create our final transformation that would be used before we send the data for training the model\n",
    "transform = torchvision.transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4743617, 0.49847862, 0.4265874 ),\n",
    "                                                                 (0.21134755, 0.19044809, 0.22679578))]\n",
    "                                          )\n",
    "crop_dataset = CropDataset(TRAIN_DIR, transform, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b6abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a weighted sampler for the images \n",
    "num_samples = len(crop_dataset.img_dataset)\n",
    "label = list(crop_dataset.img_dataset.targets)\n",
    "class_weights = [round(1.0/v,5) for k, v in dict(Counter(crop_dataset.img_dataset.targets)).items()]\n",
    "weights = [class_weights[label[i]] for i in range(num_samples)]\n",
    "\n",
    "# Now we can create a loader that will help us load images in batches for training purpose \n",
    "sampler = WeightedRandomSampler(weights, num_samples, replacement=True)\n",
    "crop_loader = DataLoader(dataset=crop_dataset.img_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         sampler=sampler, \n",
    "                         #collate_fn=crop_dataset.collate_fn\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8523fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included data from all 10939 samples\n",
      "Class distribution {17: 328, 24: 327, 25: 371, 13: 337, 18: 297, 6: 325, 30: 303, 12: 327, 31: 309, 0: 283, 15: 307, 4: 331, 10: 335, 16: 312, 32: 326, 21: 339, 1: 334, 20: 306, 33: 323, 3: 313, 7: 331, 14: 326, 5: 312, 8: 322, 26: 319, 2: 304, 22: 346, 19: 322, 9: 346, 11: 306, 27: 328, 28: 316, 29: 328, 23: 300}\n",
      "Total 10939 records processed\n"
     ]
    }
   ],
   "source": [
    "lst_target = []\n",
    "for idx, (image, target) in enumerate(crop_loader):\n",
    "    lst_target.extend(list(target.numpy()))\n",
    "    if idx%100 == 0 and idx > 0:\n",
    "        print(f'Included data from first {idx * BATCH_SIZE} samples')\n",
    "print(f'Included data from all {num_samples} samples')\n",
    "print(f'Class distribution {dict(Counter(lst_target))}')\n",
    "print(f'Total {len(lst_target)} records processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b664e8",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c90eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CropDataset(TEST_DIR, transform, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64937609",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset.img_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle= True\n",
    "                         #collate_fn=crop_dataset.collate_fn\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105dfd99",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd43890",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CropDataset(VALIDATE_DIR, transform, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(dataset=valid_dataset.img_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle= True\n",
    "                         #collate_fn=crop_dataset.collate_fn\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e074a",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_epochs, data_loader):\n",
    "    train_epochs, train_loss, train_accuracy = [], [], []\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            if (torch.cuda.is_available()):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            else:\n",
    "                inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            if i % 100 == 0 and i > 0:    \n",
    "                accu=100.*correct/total\n",
    "                mloss=running_loss / 100.\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {mloss:.3f} accuracy:{accu:.3f}')\n",
    "                train_epochs.append(epoch + 1)\n",
    "                train_loss.append(mloss)\n",
    "                train_accuracy.append(accu)\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "    return train_epochs, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec64f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct // total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy} %')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf34ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_accuracy(model, dataset, data_loader):\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {value:0 for key, value in dataset.img_dataset.class_to_idx.items()}\n",
    "    total_pred = {value:0 for key, value in dataset.img_dataset.class_to_idx.items()}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[int(label)] += 1\n",
    "                total_pred[int(label)] += 1\n",
    "\n",
    "    accuracy_per_class = {}\n",
    "    # print accuracy for each class\n",
    "    for key, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[key]\n",
    "        print(f'Accuracy for class: {key:5d} is {accuracy:.1f} %')\n",
    "        accuracy_per_class[key] = accuracy\n",
    "    \n",
    "    return accuracy_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341695bc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873208be",
   "metadata": {},
   "source": [
    "### Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0be5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a a model that can be trained for disease detection\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 9)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 6)\n",
    "        self.conv3 = nn.Conv2d(12, 18, 3)\n",
    "        self.fc1 = nn.Linear(18 * 28 * 28, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 38)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "if(torch.cuda.is_available()):\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_epochs, cust_loss, cust_accuracy = train_model(net, 10, crop_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783658dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(cust_epochs, cust_loss, 'bo', label='Training loss')\n",
    "plt.title('Training loss (Custom Model)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(cust_epochs, cust_accuracy, 'r', label='Training accuracy')\n",
    "plt.title('Training accuracy (Custom Model)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ab31f",
   "metadata": {},
   "source": [
    "#### Test Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd03f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_test_accuracy = compute_accuracy(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c491406",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_test_accuracy_per_class = compute_class_accuracy(net, test_dataset, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcb8ce",
   "metadata": {},
   "source": [
    "#### Validate Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_valid_accuracy = compute_accuracy(net, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_valid_accuracy_per_class = compute_class_accuracy(net, valid_dataset, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c72cb",
   "metadata": {},
   "source": [
    "### Transfer Learning - VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_model():\n",
    "    \n",
    "    # initialize a vgg model\n",
    "    vgg_model = models.vgg16(pretrained=True)\n",
    "    \n",
    "    # freeze all the parameters in the sequentional layer\n",
    "    for param in vgg_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # change the average pool layer\n",
    "    vgg_model.avgpool = nn.AdaptiveAvgPool2d(output_size=(7,7))\n",
    "\n",
    "    # Change the Classifier layer\n",
    "    vgg_model.classifier = nn.Sequential(nn.Flatten(),\n",
    "                                    nn.Linear(25088, 4096),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(4096, 512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(512, 34))\n",
    "    return vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f560a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "if(torch.cuda.is_available()):\n",
    "    vgg16 = vgg16.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237adde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg16, torch.zeros(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_epochs, vgg_loss, vgg_accuracy = train_model(vgg16, 5,crop_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(vgg_epochs, vgg_loss, 'bo', label='Training loss')\n",
    "plt.title('Training loss (VGG)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(vgg_epochs, vgg_accuracy, 'r', label='Training accuracy')\n",
    "plt.title('Training accuracy (VGG)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3d655",
   "metadata": {},
   "source": [
    "#### Test VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c62f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_test_accuracy = compute_accuracy(vgg16, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_test_accuracy_per_class = compute_class_accuracy(vgg16, test_dataset, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cd47d",
   "metadata": {},
   "source": [
    "#### Validate VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ee2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_valid_accuracy= compute_accuracy(vgg16, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_valid_accuracy_per_class = compute_class_accuracy(vgg16, valid_dataset, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbafc49",
   "metadata": {},
   "source": [
    "#### Save VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5df59e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg16, 'crop_disease_model_local.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddd9a5",
   "metadata": {},
   "source": [
    "### Transfer Learning - ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115f826",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628fc7c",
   "metadata": {},
   "source": [
    "#### Distribution of Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, par in enumerate(net.parameters()):\n",
    "    if(ix%2==0):\n",
    "        plt.hist(par.cpu().detach().numpy().flatten())\n",
    "        plt.title(f'Distribution of weights conencting layer \\\n",
    "{ix} to hidden layer {ix + 1}')\n",
    "        plt.show()\n",
    "    elif(ix%2==1):\n",
    "        plt.hist(par.cpu().detach().numpy().flatten())\n",
    "        plt.title(f'Distribution of biases of layer {ix}')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
