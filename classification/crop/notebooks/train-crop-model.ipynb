{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f20cf3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook captures the implementation of training an image classification model to identify if a crop is infected based on the input leaf image. This notebook will be executed in kaggle because of large data size as well as the various options we would like to explore for improving the training accuracy \n",
    "- Different Learning Rates : Warm-up LR, Step Decay LR, Cosine Decay LR, Hybrid, Annealing, Adaptive Decay LR\n",
    "- Different Cost Functions : Use Focal Loss function\n",
    "- Label Smoothening : Instead of a 0 and 1 for the labels create a continuous distribution of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f92978",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "aad0788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "88e5018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = './images/input/train/'\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40e52c",
   "metadata": {},
   "source": [
    "#### For GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59d813c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "60387ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    def __init__(self, file_path, transform=None, augment=False):\n",
    "        xform = transforms.Compose([transforms.ToTensor()]) if transform is None else transform\n",
    "        self.img_dataset = datasets.ImageFolder(file_path, transform=xform)\n",
    "        self.aug = augment\n",
    "        \n",
    "        ## augumentation initialization\n",
    "        ia.seed(241)\n",
    "\n",
    "        self.seq = iaa.Sequential([\n",
    "            iaa.Fliplr(0.5), # horizontal flips\n",
    "            iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "            # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "            # But we only blur about 50% of all images.\n",
    "            iaa.Sometimes(\n",
    "                0.5,\n",
    "                iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "            ),\n",
    "            # Strengthen or weaken the contrast in each image.\n",
    "            iaa.LinearContrast((0.75, 1.5)),\n",
    "            # Add gaussian noise.\n",
    "            # For 50% of all images, we sample the noise once per pixel.\n",
    "            # For the other 50% of all images, we sample the noise per pixel AND\n",
    "            # channel. This can change the color (not only brightness) of the\n",
    "            # pixels.\n",
    "            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "            # Make some images brighter and some darker.\n",
    "            # In 20% of all cases, we sample the multiplier once per channel,\n",
    "            # which can end up changing the color of the images.\n",
    "            iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "            # Apply affine transformations to each image.\n",
    "            # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "            iaa.Affine(\n",
    "                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                rotate=(-25, 25),\n",
    "                shear=(-8, 8)\n",
    "            )\n",
    "        ], random_order=True) # apply augmenters in random order\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.img_dataset[idx]\n",
    "        return image, label\n",
    "    \n",
    "    # this is an experimental method created to explore how the dunder methods can be modified\n",
    "    def __repr__(self, idx=0):\n",
    "        image, label = self.img_dataset[idx]\n",
    "        plt.imshow(image.permute(1,2,0))\n",
    "        return str(\"Display view of the image\")\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images, targets = list(zip(*batch))\n",
    "        images = torch.stack(images).permute(0 , 2, 3, 1)\n",
    "        if self.aug: images=self.seq.augment_images(images=images.numpy()) \n",
    "        targets = torch.tensor(targets)\n",
    "        images = torch.tensor(images).permute(0, 3, 1, 2)\n",
    "        if torch.cuda.is_available():\n",
    "            targets = targets.to(device)\n",
    "            images = images.to(device)\n",
    "        return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8926edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple___Apple_scab': 0, 'Apple___Black_rot': 1, 'Apple___Cedar_apple_rust': 2, 'Apple___healthy': 3, 'Blueberry___healthy': 4, 'Cherry_(including_sour)___Powdery_mildew': 5, 'Cherry_(including_sour)___healthy': 6, 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7, 'Corn_(maize)___Common_rust_': 8, 'Corn_(maize)___Northern_Leaf_Blight': 9, 'Corn_(maize)___healthy': 10, 'Grape___Black_rot': 11, 'Grape___Esca_(Black_Measles)': 12, 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13, 'Grape___healthy': 14, 'Peach___Bacterial_spot': 15, 'Peach___healthy': 16, 'Pepper,_bell___Bacterial_spot': 17, 'Pepper,_bell___healthy': 18, 'Potato___Early_blight': 19, 'Potato___Late_blight': 20, 'Potato___healthy': 21, 'Strawberry___Leaf_scorch': 22, 'Strawberry___healthy': 23, 'Tomato___Bacterial_spot': 24, 'Tomato___Early_blight': 25, 'Tomato___Late_blight': 26, 'Tomato___Leaf_Mold': 27, 'Tomato___Septoria_leaf_spot': 28, 'Tomato___Spider_mites Two-spotted_spider_mite': 29, 'Tomato___Target_Spot': 30, 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 31, 'Tomato___Tomato_mosaic_virus': 32, 'Tomato___healthy': 33}\n"
     ]
    }
   ],
   "source": [
    "# Here we create our final transformation that would be used before we send the data for training the model\n",
    "transform = torchvision.transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4743617, 0.49847862, 0.4265874 ),\n",
    "                                                                 (0.21134755, 0.19044809, 0.22679578))]\n",
    "                                          )\n",
    "crop_dataset = CropDataset(TRAIN_DIR, transform, True)\n",
    "print(crop_dataset.img_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d9b0a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included data from all 10939 samples\n",
      "Class distribution {22: 315, 3: 357, 33: 335, 0: 301, 7: 287, 32: 320, 19: 311, 29: 336, 11: 330, 12: 297, 17: 338, 9: 305, 25: 338, 30: 326, 21: 313, 24: 330, 8: 325, 6: 289, 10: 305, 26: 314, 23: 317, 20: 369, 31: 326, 16: 315, 1: 312, 27: 341, 14: 335, 15: 319, 2: 316, 18: 323, 4: 328, 5: 333, 28: 306, 13: 327}\n"
     ]
    }
   ],
   "source": [
    "# define a weighted sampler for the images \n",
    "lst_target = []\n",
    "num_samples = len(crop_dataset.img_dataset)\n",
    "label = list(crop_dataset.img_dataset.targets)\n",
    "class_weights = [round(1.0/v,5) for k, v in dict(Counter(crop_dataset.img_dataset.targets)).items()]\n",
    "weights = [class_weights[label[i]] for i in range(num_samples)]\n",
    "\n",
    "# Now we can create a loader that will help us load images in batches for training purpose \n",
    "sampler = WeightedRandomSampler(weights, num_samples, replacement=True)\n",
    "crop_loader = DataLoader(dataset=crop_dataset.img_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         sampler=sampler, \n",
    "                         collate_fn=crop_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2986f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (image, target) in enumerate(crop_loader):\n",
    "    lst_target.extend(list(target.numpy()))\n",
    "    if idx%100 == 0 and idx > 0:\n",
    "        print(f'Included data from first {idx * BATCH_SIZE} samples')\n",
    "print(f'Included data from all {num_samples} samples')\n",
    "print(f'Class distribution {dict(Counter(lst_target))}')\n",
    "print(f'Total {} records processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2d37a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a a model that can be trained for disease detection\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 9)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 6)\n",
    "        self.conv3 = nn.Conv2d(12, 18, 3)\n",
    "        self.fc1 = nn.Linear(18 * 28 * 28, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 38)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a2f9e",
   "metadata": {},
   "source": [
    "#### Cross Enropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "770ad4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "43ad53d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 9, 9])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([12, 6, 6, 6])\n",
      "conv2.bias \t torch.Size([12])\n",
      "conv3.weight \t torch.Size([18, 12, 3, 3])\n",
      "conv3.bias \t torch.Size([18])\n",
      "fc1.weight \t torch.Size([4096, 14112])\n",
      "fc1.bias \t torch.Size([4096])\n",
      "fc2.weight \t torch.Size([1024, 4096])\n",
      "fc2.bias \t torch.Size([1024])\n",
      "fc3.weight \t torch.Size([512, 1024])\n",
      "fc3.bias \t torch.Size([512])\n",
      "fc4.weight \t torch.Size([38, 512])\n",
      "fc4.bias \t torch.Size([38])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "34194364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}]\n"
     ]
    }
   ],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9645553",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0ac16",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.train()\n",
    "    for i, data in enumerate(crop_loader, 0):\n",
    "        if (torch.cuda.is_available()):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        else:\n",
    "            inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print(f'Processed {i} batch of 128 images')\n",
    "            \n",
    "        if i % 500 == 499:    # print every 2000 mini-batches\n",
    "            accu=100.*correct/total\n",
    "            \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 500:.3f} accuracy:{accu:.3f}')\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7e0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c58b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into dataset\n",
    "\n",
    "# observe the data\n",
    "\n",
    "# run the training module \n",
    "    # run the augmentation module\n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
